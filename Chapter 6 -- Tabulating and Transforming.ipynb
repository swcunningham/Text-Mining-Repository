{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the raw dictionary of dictionaries containing all the abstracts. Notice the need to use the encoding statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "fn = 'C:/iPython Notebook/Data/JSON/full_nano_JSON.txt'\n",
    "with open(fn, encoding = 'UTF-8') as fh:\n",
    "    corpus = json.load(fh)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing content\n",
    "\n",
    "\n",
    "Quite often, the first thing to do after having collected the data is to make a few lists to get a feel for the data. So, what are the most frequently occuring journals in the dataset? What are the most frequently occuring authors in the dataset? etc.\n",
    "\n",
    "This is a straight forward operation in Python because it provides us with a nice counter straight out of the box. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "counter = collections.Counter()\n",
    "\n",
    "for record in corpus.values():\n",
    "     counter[record['J9']] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A counter in python is an extension of the default dictonary. It comes with a couple of additional useful features. The first is the most_common method. This method returns a list with the items and their frequency. We can specify the number of most common items by using the 'n' keyword argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 1481),\n",
       " ('ABSTR PAP AM CHEM S', 414),\n",
       " ('P SOC PHOTO-OPT INS', 374),\n",
       " ('ACS NANO', 351),\n",
       " ('PROC SPIE', 346),\n",
       " ('ANGEW CHEM INT EDIT', 344),\n",
       " ('J NANOPART RES', 329),\n",
       " ('J NANOSCI NANOTECHNO', 311),\n",
       " ('NANOTECHNOLOGY', 304),\n",
       " ('INT J NANOMED', 232),\n",
       " ('APPL PHYS LETT', 230),\n",
       " ('NANO LETT', 222),\n",
       " ('MATER TODAY', 217),\n",
       " ('NANOMEDICINE-UK', 196),\n",
       " ('J AM CHEM SOC', 183),\n",
       " ('IEEE T NANOTECHNOL', 177),\n",
       " ('LANGMUIR', 171),\n",
       " ('NANOSCALE', 169),\n",
       " ('SMALL', 166),\n",
       " ('CHEM ENG NEWS', 164)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counters also can be added or substracted from one another. So imagine you have two counters, A and B, \n",
    "\n",
    "we can then write\n",
    "\n",
    "```Python\n",
    "\n",
    "c = a - b\n",
    "\n",
    "```\n",
    "\n",
    "Here we substract counter b from counter a. Implicitly here, python will loop over the items in both counters and substract the items in b from those in a. If an item does not exist in a or b, it is handled as being zero.\n",
    "\n",
    "## authors\n",
    "\n",
    "Let's move to a slightly more complicated example. Imagine we would like to count the frequency of authors in the dataset. The author information in ISI is stored under the 'AU' key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Williams, RM; Shah, J; Ng, BD; Minton, DR; Gudas, LJ; Park, CY; Heller, DA\n"
     ]
    }
   ],
   "source": [
    "# <-- would be nice to have a standard naming convention\n",
    "\n",
    "authors = corpus.get(list(corpus.keys())[0])['AU']\n",
    "print(authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, I show above the authors of the first item in the corpus. If we look at this carefully, we see that all the authors are stored in a single string. Different authors are seperated by a semicolon. So, if we want to count the frequency of authors in the dataset, we need to do some basci string handling. We need to split the string on the semicolon like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wang, BB', ' Ostrikov, K']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_authors = authors.split(';')\n",
    "list_of_authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This split operation returns a list with one or more items. As you can see, sometimes there is some white space before an author name, and we can also have some inconsistency with respect to upper and lower case use. So let us clean up the string a bit more. We are going to trim of the white space and force everything to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wang, bb', 'ostrikov, k']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_authors = [author.strip() for author in list_of_authors]\n",
    "list_of_authors = [author.lower() for author in list_of_authors]\n",
    "list_of_authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have consistent looking representaitons of the author names. This list can be used in combination with a Counter as we did when counting journals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[anonymous]', 569),\n",
       " ('webster, tj', 107),\n",
       " ('liu, y', 99),\n",
       " ('wang, j', 81),\n",
       " ('wang, y', 74),\n",
       " ('seeman, nc', 69),\n",
       " ('feng, ss', 68),\n",
       " ('li, j', 64),\n",
       " ('zhang, y', 61),\n",
       " ('li, y', 57),\n",
       " ('guo, px', 55),\n",
       " ('ferrari, m', 55),\n",
       " ('roco, mc', 53),\n",
       " ('yan, h', 52),\n",
       " ('liu, j', 50),\n",
       " ('kim, j', 47),\n",
       " ('kumar, s', 46),\n",
       " ('chen, y', 45),\n",
       " ('lee, j', 44),\n",
       " ('ostrikov, k', 42)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = collections.Counter()\n",
    "\n",
    "for record in corpus.values():\n",
    "    authors = record['AU']\n",
    "    list_of_authors = authors.split(';')\n",
    "    list_of_authors = [author.strip() for author in list_of_authors]\n",
    "    list_of_authors = [author.lower() for author in list_of_authors]    \n",
    "    \n",
    "    for author in list_of_authors:\n",
    "         counter[author] += 1\n",
    "\n",
    "counter.most_common(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a nice overview of the frequency of authors in our dataset. It is not uncommon that the same author exists under slightly different names in the dataset. Sometimes an author's first name might be written out, in other cases initials are used. At the end of this notebook, we return to this problem and demostrate a way of handling this. \n",
    "\n",
    "## countries\n",
    "\n",
    "Let's move to a more complicated example were we have to move beyond basic string handling to resolve the issue. Let's say that we would like to know the frequency of the countries in the dataset. How many articles are being published by a given country? This is a complicated issue for several reasons:\n",
    "* we need to determine the country for each author on a given paper\n",
    "* we need to count each country only once for each paper\n",
    "* how do we handle papers written by people from different countries?\n",
    "We now are going to adress these issues step by step.\n",
    "\n",
    "First, let's start with the country information. ISI provides detailed affiliation information for each author via the C1 tag.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Univ Virginia, Sch Engn & Appl Sci, Div Technol Culture & Commun, Charlottesville, VA 22904 USA\n"
     ]
    }
   ],
   "source": [
    "# <-- some python 2X in here\n",
    "# <-- you wont get the same article twice\n",
    "affil = corpus.get(list(corpus.keys())[2])['C1']\n",
    "print(affil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like with the author example, semicolons are being used as a seperator. So a naive approach would be to split on the semicolon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Univ Virginia, Sch Engn & Appl Sci, Div Technol Culture & Commun, Charlottesville, VA 22904 USA']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affil.split(';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do get a nice looking list, but if you look a bit more careful you see that it is rather messy. We have a mix of authors but also two entries that contain both an author name as well as an affiliation. We could do some more string handling to resolve this, but in this case the use of a regular expression might be much more elegant. \n",
    "\n",
    "If we look carefully at the affiliation field, we see that we have a nice pattern. There is a list of authors between square brackets, followed by the adress information.  This pattern is seperated by a semicolon. If we use a regular expression to match on this pattern, we can use this for splitting instead. So, we want to mach to the brackets and their elements in between. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Univ Virginia, Sch Engn & Appl Sci, Div Technol Culture & Commun, Charlottesville, VA 22904 USA\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "regexp = re.compile(r'\\[(.*?)\\]')\n",
    "elements = re.split(regexp, affil)\n",
    "for element in elements:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This regular expression produces nice clean results. We have a list of itemts that alternates between the list of authors that was between the square brackets and the affiliation information associated with this set of authors. Also, the first item is an empty string, so let's first remove the empty string, and then reform the list such that authors and affiliations are explicitly connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x1cbb94c8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements = [entry for entry in elements if entry]\n",
    "\n",
    "author_affil = zip(elements[0::2], elements[1::2])\n",
    "author_affil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we remove the empty string using a filter operation inside the list expression. An empty string evaluates always to False in Python and so get's dropped off. Next, we create a new list of tuples where the first item is the list of authors and the second item is their affiliation adress. We use the zip function for this, in combination with a slice notation. This slice notation starts at either 0, for the authors, or 1 for their adress. It goes to the end of the list with a stepsize of 2. \n",
    "\n",
    "The final step is to take this and reformat it such that we have a mapping between an author and an affiliation. To to this, we iterate over the new list, split the first entry by semicolon, and match each author to the associated affiliation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subramanian, balajikarthick [u' Univ Massachusetts, Ctr Hlth & Dis Res, Lowell, MA 01854 USA; ', u' Univ Massachusetts, Biomed Engn & Biotechnol Program, Lowell, MA 01854 USA; ']\n",
      "yoganathan, subbiah [u' Forsyth Inst, Boston, MA USA']\n",
      "wilson, thomas [u' Univ Massachusetts, Ctr Hlth & Dis Res, Lowell, MA 01854 USA; ']\n",
      "kotyla, tim [u' Univ Massachusetts, Ctr Hlth & Dis Res, Lowell, MA 01854 USA; ']\n",
      "kuo, fonghsu [u' Univ Massachusetts, Ctr Hlth & Dis Res, Lowell, MA 01854 USA; ', u' Univ Massachusetts, Biomed Engn & Biotechnol Program, Lowell, MA 01854 USA; ']\n",
      "nicolosi, robert [u' Univ Massachusetts, Ctr Hlth & Dis Res, Lowell, MA 01854 USA; ', u' Univ Massachusetts, Biomed Engn & Biotechnol Program, Lowell, MA 01854 USA; ']\n",
      "ada, earl [u' Univ Massachusetts, Mat Characterizat Lab, Lowell, MA 01854 USA; ']\n"
     ]
    }
   ],
   "source": [
    "author_affiliation_map = collections.defaultdict(list)\n",
    "\n",
    "for authors, affiliation in author_affil:\n",
    "    list_of_authors = authors.split(';')\n",
    "    list_of_authors = [author.strip() for author in list_of_authors]\n",
    "    list_of_authors = [author.lower() for author in list_of_authors]\n",
    "    \n",
    "    for author in list_of_authors:\n",
    "        author_affiliation_map[author].append(affiliation)\n",
    "\n",
    "for key, value in author_affiliation_map.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it might be possible for a given author to have more than one affiliation. We therefore are using a defaultdict with lists and append to this list. \n",
    "\n",
    "Now that we have a nice mapping of authors to affiliations, let's turn to the next problem: extracting the country name from the affiliation. First, let's look at some affiliations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mem Sloan Kettering Canc Ctr, New York, NY 10065 USA; \n",
      " Weill Cornell Grad Sch Med Sci, New York, NY 10065 USA; \n",
      " Weill Cornell Med Coll, Dept Pharmacol, New York, NY 10065 USA; \n",
      " Weill Cornell Med Coll, Dept Cell & Dev Biol, New York, NY 10065 USA\n",
      " Kanazawa Univ, Grad Sch Nat Sci & Technol, Div Math & Phys Sci, Kanazawa, Ishikawa 9201192, Japan; \n",
      " Natl Inst Adv Ind Sci & Technol, Res Inst Computat Sci, Tsukuba, Ibaraki 3058568, Japan\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    affil = corpus.get(list(corpus.keys())[i])['C1']\n",
    "    elements = re.split(regexp, affil)\n",
    "    elements = [entry for entry in elements if entry]\n",
    "    for entry in elements[1::2]:\n",
    "        print(entry)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the country is at the end of the string, but sometimes there is some non-alphanumeric stuff there as well. So the first step is to remove that. Next, we can split it on a comma and only keep the last element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NY 10065 USA\n",
      "NY 10065 USA\n",
      "NY 10065 USA\n",
      "NY 10065 USA\n",
      "Japan\n",
      "Japan\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    affil = corpus.get(list(corpus.keys())[i])['C1']\n",
    "    elements = re.split(regexp, affil)\n",
    "    elements = [entry for entry in elements if entry]\n",
    "    for entry in elements[1::2]:\n",
    "        entry = entry.strip('.; ')\n",
    "        country = entry.split(', ')[-1]\n",
    "        print(country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we are making progress, but the USA format is different from that of the other countries. It contains the state, sometimes a postal code, and the country. We only need the last part and not the rest. A naive solution would be to split on white space, but this would break for country names like South Korea or Peoples Republic of China. Instead, we are going to use a simple regular expression. The patter is simple, two capital letters, optionally followed by 5 numbers, followed by three capital letters. These numbers don't always occur, they are either there or not. This implies also that the second space is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USA\n",
      "USA\n",
      "USA\n",
      "USA\n",
      "Japan\n",
      "Japan\n"
     ]
    }
   ],
   "source": [
    "country_regexp = re.compile(r'([A-Z]{2})\\s([\\d]{0,5})\\s?([A-Z]{3})')\n",
    "\n",
    "for i in range(4):\n",
    "    affil = corpus.get(list(corpus.keys())[i])['C1']\n",
    "    elements = re.split(regexp, affil)\n",
    "    elements = [entry for entry in elements if entry]\n",
    "    for entry in elements[1::2]:\n",
    "        entry = entry.strip('.; ')\n",
    "        country = entry.split(', ')[-1]\n",
    "        match = re.match(country_regexp, country)\n",
    "        if match:\n",
    "            country = match.group(3)\n",
    "        \n",
    "        print(country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinks are clearing up nicely. So far, we have tested on the first 4 records (note the 4 as an argument to range). Let's expand the number of records a bit, and avoid showing countries multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "South Korea\n",
      "Russia\n",
      "France\n",
      "South Africa\n",
      "Israel\n",
      "Australia\n",
      "Japan\n",
      "Switzerland\n",
      "Singapore\n",
      "Greece\n",
      "Hungary\n",
      "Peoples R China\n",
      "India\n",
      "Iran\n",
      "Slovenia\n",
      "Czech Republic\n",
      "USA\n",
      "Sweden\n",
      "Serbia\n",
      "Egypt\n",
      "Italy\n",
      "Germany\n",
      "Netherlands\n"
     ]
    }
   ],
   "source": [
    "countries = set()\n",
    "\n",
    "for i in range(50):\n",
    "    affil = corpus.get(list(corpus.keys())[i])['C1']\n",
    "    elements = re.split(regexp, affil)\n",
    "    elements = [entry for entry in elements if entry]\n",
    "    for entry in elements[1::2]:\n",
    "        entry = entry.strip('.; ')\n",
    "        country = entry.split(', ')[-1]\n",
    "        match = re.match(country_regexp, country)\n",
    "        if match:\n",
    "            country = match.group(3)\n",
    "        \n",
    "        countries.add(country)\n",
    "        \n",
    "for entry in countries:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks alright. We tested the regular expression on the first 50 records, and no strange results show up. Let's therefore combine the handing of the affiliation with the matching of the affiliation with authors. To do this, we are introducting a new function get_country which accepts the affiliation and returns the country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'ng, brandon d.': ['USA'], 'heller, daniel a.': ['USA', 'USA'], 'shah, janki': ['USA'], 'williams, ryan m.': ['USA'], 'gudas, lorraine j.': ['USA'], 'minton, denise r.': ['USA'], 'park, christopher y.': ['USA', 'USA']})\n",
      "defaultdict(<class 'list'>, {'saito, mineo': ['Japan'], 'ishii, fumiyuki': ['Japan', 'Japan'], 'sawada, keisuke': ['Japan']})\n"
     ]
    }
   ],
   "source": [
    "country_regexp = re.compile(r'([A-Z]{2})\\s([\\d]{0,5})\\s?([A-Z]{3})')\n",
    "affil_regexp = re.compile(r'\\[(.*?)\\]')\n",
    "\n",
    "def get_country(affiliation):\n",
    "    entry = affiliation.strip('.; ')\n",
    "    country = entry.split(', ')[-1]\n",
    "    match = re.match(country_regexp, country)\n",
    "    if match:\n",
    "        country = match.group(3)\n",
    "    return country\n",
    "\n",
    "def process_affiliations(record):\n",
    "    affil = record['C1']\n",
    "    elements = re.split(regexp, affil)\n",
    "    elements = [entry for entry in elements if entry]\n",
    "    \n",
    "    author_affil = zip(elements[0::2], elements[1::2])\n",
    "\n",
    "    author_affiliation_map = collections.defaultdict(list)\n",
    "\n",
    "    for authors, affiliation in author_affil:\n",
    "        list_of_authors = authors.split(';')\n",
    "        list_of_authors = [author.strip() for author in list_of_authors]\n",
    "        list_of_authors = [author.lower() for author in list_of_authors]\n",
    "\n",
    "        for author in list_of_authors:\n",
    "            country = get_country(affiliation)\n",
    "            author_affiliation_map[author].append(country)\n",
    "    return author_affiliation_map\n",
    "\n",
    "for i in range(2):\n",
    "    record = corpus.get(list(corpus.keys())[i])\n",
    "    \n",
    "    print(process_affiliations(record))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these results, we could start doing a simple count, like we have done in the previous examples. However, this would result counting the same paper multiple times if the authors come from the same country. We need to do something else instead. The easiest step is to first make a matrix of records by countries. This indicates for each record the country of origin of the various authors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counted_records = {}\n",
    "\n",
    "for key, record in corpus.items():\n",
    "    affiliation_map = process_affiliations(record)\n",
    "    \n",
    "    counter = collections.Counter()\n",
    "    for author, countries in affiliation_map.items():\n",
    "        \n",
    "        for country in countries:\n",
    "            counter[country] += 1 \n",
    "    counted_records[key] = counter\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USA</th>\n",
       "      <th>Japan</th>\n",
       "      <th>Iran</th>\n",
       "      <th>Czech Republic</th>\n",
       "      <th>Egypt</th>\n",
       "      <th>Switzerland</th>\n",
       "      <th>Peoples R China</th>\n",
       "      <th>Germany</th>\n",
       "      <th>Sweden</th>\n",
       "      <th>South Korea</th>\n",
       "      <th>...</th>\n",
       "      <th>Yuchun</th>\n",
       "      <th>Jay</th>\n",
       "      <th>Azerbaijan</th>\n",
       "      <th>J. P</th>\n",
       "      <th>B. S</th>\n",
       "      <th>Niger</th>\n",
       "      <th>S. G</th>\n",
       "      <th>J. B</th>\n",
       "      <th>R. O</th>\n",
       "      <th>Morocco</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>WOS:000202991501296</th>\n",
       "      <td> 5</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WOS:000202995300850</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WOS:000202995300852</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WOS:000202998600001</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WOS:000203538900008</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WOS:000205747600003</th>\n",
       "      <td> 3</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WOS:000205747600006</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WOS:000205747600009</th>\n",
       "      <td> 6</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WOS:000205747600011</th>\n",
       "      <td> 4</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WOS:000205747800001</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     USA  Japan  Iran  Czech Republic  Egypt  Switzerland  \\\n",
       "WOS:000202991501296    5      0     0               0      0            0   \n",
       "WOS:000202995300850    0      0     0               0      0            0   \n",
       "WOS:000202995300852    0      0     0               0      0            0   \n",
       "WOS:000202998600001    0      0     0               0      0            0   \n",
       "WOS:000203538900008    0      0     0               0      0            0   \n",
       "WOS:000205747600003    3      0     0               0      0            0   \n",
       "WOS:000205747600006    0      0     0               0      0            0   \n",
       "WOS:000205747600009    6      0     0               0      0            0   \n",
       "WOS:000205747600011    4      0     0               0      0            0   \n",
       "WOS:000205747800001    0      0     0               0      0            0   \n",
       "\n",
       "                     Peoples R China  Germany  Sweden  South Korea   ...     \\\n",
       "WOS:000202991501296                0        0       0            0   ...      \n",
       "WOS:000202995300850                0        0       0            0   ...      \n",
       "WOS:000202995300852                0        0       0            0   ...      \n",
       "WOS:000202998600001                0        0       0            0   ...      \n",
       "WOS:000203538900008                0        0       0            0   ...      \n",
       "WOS:000205747600003                0        0       0            0   ...      \n",
       "WOS:000205747600006                0        0       0            0   ...      \n",
       "WOS:000205747600009                0        0       0            0   ...      \n",
       "WOS:000205747600011                0        0       0            0   ...      \n",
       "WOS:000205747800001                0        0       0            0   ...      \n",
       "\n",
       "                     Yuchun  Jay  Azerbaijan  J. P  B. S  Niger  S. G  J. B  \\\n",
       "WOS:000202991501296       0    0           0     0     0      0     0     0   \n",
       "WOS:000202995300850       0    0           0     0     0      0     0     0   \n",
       "WOS:000202995300852       0    0           0     0     0      0     0     0   \n",
       "WOS:000202998600001       0    0           0     0     0      0     0     0   \n",
       "WOS:000203538900008       0    0           0     0     0      0     0     0   \n",
       "WOS:000205747600003       0    0           0     0     0      0     0     0   \n",
       "WOS:000205747600006       0    0           0     0     0      0     0     0   \n",
       "WOS:000205747600009       0    0           0     0     0      0     0     0   \n",
       "WOS:000205747600011       0    0           0     0     0      0     0     0   \n",
       "WOS:000205747800001       0    0           0     0     0      0     0     0   \n",
       "\n",
       "                     R. O  Morocco  \n",
       "WOS:000202991501296     0        0  \n",
       "WOS:000202995300850     0        0  \n",
       "WOS:000202995300852     0        0  \n",
       "WOS:000202998600001     0        0  \n",
       "WOS:000203538900008     0        0  \n",
       "WOS:000205747600003     0        0  \n",
       "WOS:000205747600006     0        0  \n",
       "WOS:000205747600009     0        0  \n",
       "WOS:000205747600011     0        0  \n",
       "WOS:000205747800001     0        0  \n",
       "\n",
       "[10 rows x 129 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict(counted_records, orient='index')\n",
    "df = df.fillna(0)\n",
    "df[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <-- something went wrong here; it looks like some of the addresses and names got mixed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now almost there. We have a nice matrix that specifies the frequency of countries for each record. To count a paper for a given paper only once, we can use logical indexing. If the frequency is higher than 1, set it to one. Next, sum across columsn to produce the frequency of countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "USA                5556\n",
       "Peoples R China    2237\n",
       "Germany            1132\n",
       "India              1006\n",
       "England             853\n",
       "Italy               830\n",
       "Japan               788\n",
       "France              718\n",
       "Spain               610\n",
       "South Korea         596\n",
       "dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df>1] = 1\n",
    "country_freqs = df.sum()\n",
    "country_freqs.sort(ascending=False)\n",
    "country_freqs[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look carefully at the above code. The sum operation returns a series. This series we sort in descending order to get the most frequently occuring countries.\n",
    "\n",
    "As you might recall, there were three issues we had to face when determining the number of articles per country. These were \n",
    "* determine the country for each author on a given paper\n",
    "* count each country only once for each paper\n",
    "* how do we handle papers written by people from different countries? \n",
    "We addressed the first issue through the careful splitting of the string, first to match each author with the correct affilition, and second to extract the country from the affiliation. The second issue we addressed through logical indexing on the pandas dataframe. In the sum across the columsn of the data frame, we implicitly made an assumption with respect to the third item: we attributed an article with multiple countries once to each country. This is an easy solution, but be aware of the fact that there is quite a literature on fractionating instead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allenby b, 2001, ieee technology soc, v19, p10\n",
      "anderson j. r., 1983, architecture cogniti\n",
      "baird d., 1999, perspectives sci, v7, p231, doi 10.1162/posc.1999.7.2.231\n",
      "bechtel w., 1991, connectionism mind i\n",
      "bijker w.e., 1995, bicycles bakelites b\n",
      "bowker geoffrey c., 1999, sorting things class\n",
      "chi mth, 1981, cognitive sci, v5, p121, doi 10.1207/s15516709cog0502_2\n",
      "collins hm, 2002, soc stud sci, v32, p235, doi 10.1177/0306312702032002003\n",
      "dreyfus h. l., 1986, mind machine power h\n",
      "epstein s, 1995, sci technol hum val, v20, p408, doi 10.1177/016224399502000402\n",
      "ericsson ka, 1994, am psychol, v49, p725, doi 10.1037/0003-066x.49.8.725\n",
      "friedman t., 1999, lexus olive tree\n",
      "galison peter, 1997, image logic mat cult\n",
      "goodman peter s., 2003, washington post 0104, pa01\n",
      "gorman m. e., 2002, j technology transfe, v27, p219, doi 10.1023/a:1015672119590\n",
      "gorman me, 2002, j eng educ, v91, p339\n",
      "gorman me, 1997, soc stud sci, v27, p583, doi 10.1177/030631297027004002\n",
      "gorman m.e., 2000, ethical env challeng\n",
      "gorman me, 2002, sci technol hum val, v27, p499, doi 10.1177/016224302236179\n",
      "hughes thomas, 1998, rescuing prometheus\n",
      "jasanoff sheila, 1995, sci bar law sci tech\n",
      "johnson m, 1993, moral imagination\n",
      "koen bv, 1985, definition eng metho\n",
      "raham lr, 1993, ghost executed eng t\n",
      "roco mc, 2002, int j eng educ, v18, p488\n",
      "rodgers e., 1996, flying high story bo\n",
      "scott j. c., 1998, seeing like state ce\n",
      "sen a., 1999, dev freedom\n",
      "sen a, 1994, new york rev books, v41, p62\n",
      "vincenti w. g., 1990, what eng know they k\n",
      "wegner d. m., 1986, theories group behav, p185\n"
     ]
    }
   ],
   "source": [
    "cites = corpus.get(list(corpus.keys())[2])['CR']\n",
    "cites = cites.split('; ')\n",
    "for entry in cites:\n",
    "    print(entry.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group by\n",
    "\n",
    "\n",
    "Often we would like to combine information from different fields on a record. For example, we might want to now the frequency with which countries publish in different journals. That is, we want a matrix of country by journal. This requires combining the information from the journal field with the information from the affiliation field. It is generally convenient to use pandas in this. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counted_records = {}\n",
    "\n",
    "for key, record in corpus.items():\n",
    "    affiliation_map = process_affiliations(record)\n",
    "    \n",
    "    counter = collections.Counter()\n",
    "    for author, countries in affiliation_map.items():\n",
    "        \n",
    "        for country in countries:\n",
    "            counter[country] += 1 \n",
    "    counted_records[key] = counter\n",
    "    \n",
    "record_by_country = pd.DataFrame.from_dict(counted_records, orient='index')\n",
    "record_by_country = record_by_country.fillna(0)\n",
    "record_by_country[record_by_country>1] = 1\n",
    "\n",
    "keys = {key:record['J9'] for key, record in corpus.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      AAPS J  AAPS PHARMSCITECH  AASRI PROC  AATCC REV\n",
      "USA              182       6                  6           0          0\n",
      "Japan             18       0                  0           0          0\n",
      "Iran              19       0                  0           0          0\n",
      "Czech Republic    35       0                  0           0          0\n",
      "Egypt              2       0                  0           0          0\n",
      "Switzerland        7       0                  0           0          0\n",
      "Peoples R China   53       0                  1           0          2\n",
      "Germany           28       0                  0           0          0\n",
      "Sweden             2       0                  0           0          0\n",
      "South Korea       12       0                  0           0          0\n"
     ]
    }
   ],
   "source": [
    "country_by_journals = record_by_country.groupby(keys).sum().T\n",
    "print(country_by_journals.ix[0:10,0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above group by example, we are grouping on a unique article identifier, namely the journal. It is also possible to use group by in combination with non unique article identifiers like stop words. However, to do so is a bit more complicated. Imagine we want to profile countries by the subject categories of articles published by authors from those countries. The problem we now face is that an article can have more than one subject category, which creates a problem with group by. The solution is to make to matrices: article by country, and article by subject category. Next, we merge these two. After the merge, we can use a group by. Let's go through this step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    SC                   UT\n",
      "0                            Chemistry  WOS:000352750200022\n",
      "1  Science & Technology - Other Topics  WOS:000352750200022\n",
      "2                    Materials Science  WOS:000352750200022\n",
      "3                              Physics  WOS:000352750200022\n",
      "4                              Physics  WOS:000257272600028\n"
     ]
    }
   ],
   "source": [
    "sc_recs = []\n",
    "\n",
    "for key, rec in corpus.items():\n",
    "    subject_categories = rec['SC'].split('; ')\n",
    "    for sc in subject_categories:\n",
    "        sc_recs.append({'UT':key, 'SC':sc})\n",
    "df_sc = pd.DataFrame(sc_recs)\n",
    "print(df_sc[0:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we now have created two dataframes. The first contains the records indexed by country. The second contains the records indexed by subject category. \n",
    "\n",
    "We can now combine these two. This is knowns as a merge in pandas. In the merge function, we specify a left hand and right hand dataframe that we want to merge. We also need to specify what it is that we want to merge on. In this particular case, we would like to merge on the unique article identifier. This will result in a new dataframe which combines the information from the left and right dataframe. Because we are joining on the UT tag and the subject category dataframe contains specific UT tags more than once, we end up with a new data frame where a given article can also occur more than once.\n",
    "\n",
    "In order to merge on the UT tag, we first modify the country dataframe we produced earlier. We are moving the current index which is the UT tag to a seperate column and make sure this new column has the correct label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UT</th>\n",
       "      <th>USA</th>\n",
       "      <th>Japan</th>\n",
       "      <th>Iran</th>\n",
       "      <th>Czech Republic</th>\n",
       "      <th>Egypt</th>\n",
       "      <th>Switzerland</th>\n",
       "      <th>Peoples R China</th>\n",
       "      <th>Germany</th>\n",
       "      <th>Sweden</th>\n",
       "      <th>...</th>\n",
       "      <th>Yuchun</th>\n",
       "      <th>Jay</th>\n",
       "      <th>Azerbaijan</th>\n",
       "      <th>J. P</th>\n",
       "      <th>B. S</th>\n",
       "      <th>Niger</th>\n",
       "      <th>S. G</th>\n",
       "      <th>J. B</th>\n",
       "      <th>R. O</th>\n",
       "      <th>Morocco</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> WOS:000202991501296</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> WOS:000202995300850</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> WOS:000202995300852</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> WOS:000202998600001</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> WOS:000203538900008</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    UT  USA  Japan  Iran  Czech Republic  Egypt  Switzerland  \\\n",
       "0  WOS:000202991501296    1      0     0               0      0            0   \n",
       "1  WOS:000202995300850    0      0     0               0      0            0   \n",
       "2  WOS:000202995300852    0      0     0               0      0            0   \n",
       "3  WOS:000202998600001    0      0     0               0      0            0   \n",
       "4  WOS:000203538900008    0      0     0               0      0            0   \n",
       "\n",
       "   Peoples R China  Germany  Sweden   ...     Yuchun  Jay  Azerbaijan  J. P  \\\n",
       "0                0        0       0   ...          0    0           0     0   \n",
       "1                0        0       0   ...          0    0           0     0   \n",
       "2                0        0       0   ...          0    0           0     0   \n",
       "3                0        0       0   ...          0    0           0     0   \n",
       "4                0        0       0   ...          0    0           0     0   \n",
       "\n",
       "   B. S  Niger  S. G  J. B  R. O  Morocco  \n",
       "0     0      0     0     0     0        0  \n",
       "1     0      0     0     0     0        0  \n",
       "2     0      0     0     0     0        0  \n",
       "3     0      0     0     0     0        0  \n",
       "4     0      0     0     0     0        0  \n",
       "\n",
       "[5 rows x 130 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_country = record_by_country.reset_index(level=0)\n",
    "df_country = df_country.rename(columns = {'index':'UT'})\n",
    "df_country[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to dataframes, both of which have a UT column. So let's merge the two data frames on the UT column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     UT  USA  Japan  Iran  Czech Republic\n",
      "0   WOS:000202991501296    1      0     0               0\n",
      "1   WOS:000202995300850    0      0     0               0\n",
      "2   WOS:000202995300852    0      0     0               0\n",
      "3   WOS:000202998600001    0      0     0               0\n",
      "4   WOS:000202998600001    0      0     0               0\n",
      "5   WOS:000202998600001    0      0     0               0\n",
      "6   WOS:000203538900008    0      0     0               0\n",
      "7   WOS:000203538900008    0      0     0               0\n",
      "8   WOS:000205747600003    1      0     0               0\n",
      "9   WOS:000205747600003    1      0     0               0\n",
      "10  WOS:000205747600006    0      0     0               0\n"
     ]
    }
   ],
   "source": [
    "df = pd.merge(df_country, df_sc, on=['UT'])\n",
    "print(df.ix[0:10, 0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, the aim was to produce an country by subject category matrix. We are almost there. We now can use groupby on the country column to produce the desired grouping. Before we do so, it might be useful to first drop the index column because we no longer need it. For this, we can use the drop method on the dataframe. We can specify here the name of the column we would like to drop and the axis along which we want to drop. Since we are not going to need the dataframe with the index column any more, we do the operation in place. That is, we modify the dataframe itself, rather than copying all the data to a new dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.drop('UT', axis=1, inplace='True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step we now do the group by on authors and sum up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SC                  Acoustics  Agriculture  Allergy  Anatomy & Morphology\n",
      "USA              0          6           25        0                     0\n",
      "Japan            0          0            0        1                     0\n",
      "Iran             0          0            4        0                     0\n",
      "Czech Republic   1          0            0        0                     1\n",
      "Egypt            0          0            0        0                     0\n",
      "Switzerland      1          0            1        0                     0\n",
      "Peoples R China  0          1           11        0                     2\n",
      "Germany          1          1            7        0                     1\n",
      "Sweden           0          0            0        0                     0\n",
      "South Korea      0          0            3        0                     0\n"
     ]
    }
   ],
   "source": [
    "df_country_sc = df.groupby('SC').sum()\n",
    "print(df_country_sc.T.ix[0:10,0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cleaning the data\n",
    "\n",
    "It is almost always necessary to perform some data cleaning at some stage when performing text mining. For example, the same person might be present in the data set under different names; the same organization might be present under slightly different names; or words might be not particularly relevant for the analysis. Data cleaning can be done at different stages of the analysis. For example, in the previous chapter, we did not index a set of stopwords. This represents a form of early cleaning. Alternatively, we can use the drop function to drop columns from the dataframe at a later stage of the analysis. \n",
    "\n",
    "As an example, let's start with making a record by author dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baglioni, p      20\n",
      "kumar, r         30\n",
      "wood, j          32\n",
      "kim, hj          17\n",
      "yoshida, y       16\n",
      "wang, zl         38\n",
      "kim, h           23\n",
      "kim, k           29\n",
      "jiang, l         25\n",
      "chen, y          45\n",
      "park, j          23\n",
      "liu, h           24\n",
      "gordon, r        16\n",
      "ostrikov, k      42\n",
      "farokhzad, oc    22\n",
      "langer, r        38\n",
      "lee, s           28\n",
      "park, jh         16\n",
      "park, sh         24\n",
      "ferrari, m       55\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# let's count the authors\n",
    "# because we don't want to index all authors\n",
    "# also keep a global counter\n",
    "authors_counters = {}\n",
    "global_counter = collections.Counter()\n",
    "for key, record in corpus.items():\n",
    "    authors = record['AU']\n",
    "    list_of_authors = authors.split('; ')\n",
    "    list_of_authors = [author.strip() for author in list_of_authors]\n",
    "    list_of_authors = [author.lower() for author in list_of_authors]    \n",
    "    \n",
    "    counter = collections.Counter()\n",
    "    \n",
    "    for author in list_of_authors:\n",
    "        counter[author] += 1\n",
    "        global_counter[author] += 1\n",
    "    authors_counters[key] = counter\n",
    "\n",
    "# get the top 250 most frequenlty occuring oauthors\n",
    "top_250 = global_counter.most_common(n=250)\n",
    "top_250 = collections.Counter(dict(top_250))\n",
    "\n",
    "# for all records only keep authors in the top 250\n",
    "indexed_authors = {}\n",
    "for key, value in authors_counters.items():\n",
    "    # we can do an intersect on the keys of two\n",
    "    # dictionaries\n",
    "    intersect = value & top_250\n",
    "    indexed_authors[key] = intersect\n",
    "\n",
    "# turn it into a dataframe\n",
    "rec_authors = pd.DataFrame.from_dict(indexed_authors, orient='index')\n",
    "rec_authors.fillna(0, inplace=True)\n",
    "\n",
    "print(rec_authors.sum()[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as an example, let's say that we discover that the authors labelled 'yang, l' and 'yang, h' are actually the same. In that case, we would like to combine the two into a single row. This can again be accomplished as a simple group by operation. However, rather than producing the column on which to group up front as we did with the country by journal example, or by including it as column in the dataframe as we did with the country by subject category example, here we are using a function. This function receives the index for a given row or the column label, and should return the group to which that index or column belongs. \n",
    "\n",
    "This function can be done as a simply lookup in a thesaurus. The thesaurus is a dict with the various author names as key, and the correct name as value. The function tries to return a value from the thesaurus. If this fails with a key error, this means that the index is not in the thesaurus and should be treated as its own group instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.0\n"
     ]
    }
   ],
   "source": [
    "thesaurus = {'yang, l': 'yang, h'}\n",
    "\n",
    "def thesaurus_lookup(author):\n",
    "    try:\n",
    "        return thesaurus[author]\n",
    "    except KeyError:\n",
    "        return author\n",
    "\n",
    "rec_authors = rec_authors.groupby(thesaurus_lookup, axis=1).sum()\n",
    "print(rec_authors.sum()['yang, h'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the best point for cleaning the data depends on the exact use case. Removing stop words is a typical thing, which is easily accomplished early. In contrast, cleaning data by merging columns or rows is often something that you discovery somewhere in the data analysis process. In those cases, it is easier to perform the cleaning later once it is discovered that it is needed. A second argument for cleaning later rather than early is that you keep your data in its original form as long as possible."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(NB: Need to save some standard information products for later use. Chapter 7 will use papers x indexed words. Nations x subject categories. Journal x Journal citation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
